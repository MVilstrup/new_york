{
 "metadata": {
  "name": "elevation_gp"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This notebook uses GPy (Gaussian Processes module) to perform an advanced type of spatial regression (also known as kriging).\n",
      "# We take elevation datapoints from OpenStreetMap and infer the surface contour of Britain."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# load the modules etc\n",
      "%pylab inline\n",
      "import numpy as np\n",
      "import pylab as pb\n",
      "import GPy\n",
      "import csv\n",
      "from operator import itemgetter\n",
      "import json\n",
      "import cPickle as pickle"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here we load the OSM elevation data, i.e. a set of nodes with the \"ele\" tag. Note that OSM is freeform so we have to be a bit careful about unexpected values.\n",
      "picklepath = 'gb_ele.pickle'\n",
      "try:\n",
      "    # cache of data we already parsed\n",
      "    eledata = pickle.load(open(picklepath, 'rb'))\n",
      "    print \"Loaded eledata from pickle\"\n",
      "except:\n",
      "    with open('gb_ele.json', 'r') as fp:\n",
      "        eledata = []\n",
      "        failedcount = 0\n",
      "        for item in json.load(fp)[u'elements']:\n",
      "            try:\n",
      "                eletag = item[u'tags'][u'ele']\n",
      "                if eletag[-3:]==' ft':\n",
      "                    eletag = float(eletag[:-3]) / 3.281\n",
      "                elif eletag[-2:]==' m':\n",
      "                    eletag = float(eletag[:-2])\n",
      "                elif eletag[-1:]=='m':\n",
      "                    eletag = float(eletag[:-1])\n",
      "                else:\n",
      "                    eletag = float(eletag)\n",
      "                eledata.append([item[u'lon'], item[u'lat'], eletag])\n",
      "            except ValueError:\n",
      "                # Typically this is weirdly-expressed ele data such as \"floor 1\"\n",
      "                #print \"Failed to decode value: %s\" % (item[u'tags'][u'ele'])\n",
      "                failedcount += 1\n",
      "            except KeyError:\n",
      "                # Typically this is things like relations rather than nodes\n",
      "                #print \"KeyError: %s\" % str(item)\n",
      "                failedcount += 1\n",
      "    print \"Loaded %i ele vals, failed to load %i\" % (len(eledata), failedcount)\n",
      "    if len(eledata)>0:\n",
      "        pickle.dump(eledata, open(picklepath, 'wb'), -1)\n",
      "np.random.shuffle(eledata)\n",
      "\n",
      "ele_scaler = np.sqrt(np.mean([item[2]**2 for item in eledata])) # This (the RMS) is the std, but centred around zero, not the mean. (We want zero to stay zero.)\n",
      "print \"ele_scaler is %g\" % ele_scaler\n",
      "eledata = [[x[0], x[1], x[2]/ele_scaler] for x in eledata] # rescale the elevations to RMS 1\n",
      "\n",
      "minlat = np.min([item[1] for item in eledata])\n",
      "minlon = np.min([item[0] for item in eledata])\n",
      "maxlat = np.max([item[1] for item in eledata])\n",
      "maxlon = np.max([item[0] for item in eledata])\n",
      "print \"If data was on a square grid, the space between would be %g\" % ((maxlat-minlat)*(maxlon-minlon)/np.sqrt(len(eledata)))\n",
      "fig = pb.figure()\n",
      "fig.set_size_inches(8, 12)\n",
      "pb.plot([item[0] for item in eledata], [item[1] for item in eledata], '.')\n",
      "pb.title(\"OpenStreetMap elevation point data for Britain. (This image (c) Dan Stowell, CC-BY. The underlying data is (c) OpenStreetMap contributors, ODbL.)\", fontsize=\"x-small\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let's also load the coastline, but keep it separate so we can mix it in to our liking (there are many more coastline points than ele points)\n",
      "picklepath = 'gb_coastline.pickle'\n",
      "try:\n",
      "    # cache of data we already parsed\n",
      "    coastdata = pickle.load(open(picklepath, 'rb'))\n",
      "    print \"Loaded coastdata from pickle\"\n",
      "except:\n",
      "    with open('gb_coastline.json', 'r') as fp:\n",
      "        coastdata = []\n",
      "        failedcount = 0\n",
      "        for item in json.load(fp)[u'elements']:\n",
      "            try:\n",
      "                coastdata.append([item[u'lon'], item[u'lat'], 0.0])\n",
      "            except:\n",
      "                #print \"Failed to decode value: %s\" % (item[u'tags'][u'ele'])\n",
      "                failedcount += 1\n",
      "    print \"Loaded %i coastline points, failed to load %i\" % (len(coastdata), failedcount)\n",
      "    pickle.dump(coastdata, open(picklepath, 'wb'), -1)\n",
      "np.random.shuffle(coastdata)\n",
      "fig = pb.figure()\n",
      "fig.set_size_inches(8, 12)\n",
      "pb.plot([item[0] for item in coastdata[:10000]], [item[1] for item in coastdata[:10000]], '.')\n",
      "pb.title(\"A random sample of OpenStreetMap coastline point data for Britain. (This image (c) Dan Stowell, CC-BY. The underlying data is (c) OpenStreetMap contributors, ODbL.)\", fontsize=\"x-small\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Here we do an overall fit for a random subset of the data.\n",
      "# It will not be very good, since our data will be too sparse to cover the UK, \n",
      "# though if we make it denser the computation time grows as n^3 so we can't simply do that.\n",
      "k = GPy.kern.exponential(input_dim=2, variance=1., lengthscale=0.1)  # Or maybe Matern32?\n",
      "trimto = 2000\n",
      "trimcoast = 500\n",
      "Xin = np.array([[item[0], item[1]] for item in (eledata[:trimto] + coastdata[:trimcoast])])\n",
      "Yin = np.array([[item[2]]          for item in (eledata[:trimto] + coastdata[:trimcoast])])\n",
      "m = GPy.models.GPRegression(Xin, Yin, k, normalize_Y=True)\n",
      "fig = pb.figure()\n",
      "fig.set_size_inches(8, 12)\n",
      "m.plot(fignum=fig.number)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Next: divide and conquer the task. run a set of lapped 1-degree-square patches, then sum them up.\n",
      "\n",
      "# The first job (this code block) is to optimise the parameters we'll be using.\n",
      "\n",
      "# all these values are in degrees\n",
      "patchsize = 2\n",
      "resn_lon = 0.025\n",
      "resn_lat = 0.0125\n",
      "totsize_lon = 9\n",
      "totsize_lat = 8\n",
      "corner_lon = -6.5\n",
      "corner_lat = 50.5\n",
      "\n",
      "loncens = np.arange(corner_lon, corner_lon + totsize_lon, patchsize * 0.5)\n",
      "latcens = np.arange(corner_lat, corner_lat + totsize_lat, patchsize * 0.5)\n",
      "\n",
      "# for each patch, a random subset will be used, in order to limit the computation - there are some really dense bits that would otherwise dominate the time taken\n",
      "maxpointsperpatch = 1000\n",
      "# we will infer the shape in overlapped patches, and so to sum them back together again we use a 2D Hann window to \"crossfade\" the patches\n",
      "window2d = np.outer(np.hanning(int(patchsize/resn_lon)), np.hanning(int(patchsize/resn_lat)))\n",
      "\n",
      "patchedimage = np.zeros((int((totsize_lon+patchsize) / resn_lon), int((totsize_lat+patchsize) / resn_lat)))\n",
      "\n",
      "lenlimit = min(len(eledata), len(coastdata)) # we'll make sure neither the coast nor the others are over-represented\n",
      "allpoints = eledata[:lenlimit] + coastdata[:lenlimit]\n",
      "\n",
      "# Choose a covariance kernel\n",
      "#k = GPy.kern.Matern32(input_dim=2, variance=1., lengthscale=0.1) + GPy.kern.bias(2)  # tested 0.01 and it was awful fragmentyness, 0.05 meh too, 0.1 good though a bit blobby\n",
      "k = GPy.kern.exponential(input_dim=2, variance=1., lengthscale=0.15) + GPy.kern.bias(2)\n",
      "\n",
      "# First we will do a fit to a reasonably representative patch of the UK, and optimise the kernel parameters using it\n",
      "loncen = -1\n",
      "latcen = 53\n",
      "lonmin = loncen - (patchsize * 0.5)\n",
      "lonmax = loncen + (patchsize * 0.5)\n",
      "latmin = latcen - (patchsize * 0.5)\n",
      "latmax = latcen + (patchsize * 0.5)\n",
      "curpoints = filter(lambda p: p[0]>lonmin and p[0]<lonmax and p[1]>latmin and p[1]<latmax, allpoints)\n",
      "np.random.shuffle(curpoints)\n",
      "print len(curpoints)\n",
      "curpoints = curpoints[:maxpointsperpatch]\n",
      "print len(curpoints)\n",
      "# fit a gp to this area\n",
      "Xin = np.array([[item[0], item[1]] for item in curpoints])\n",
      "Yin = np.array([[item[2]]          for item in curpoints])\n",
      "m = GPy.models.GPRegression(Xin, Yin, k)\n",
      "m.optimize(messages=1)\n",
      "# NOTE: this will run for a while. You can stop it at any time and get improved-but-not-locally-optimal params"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print m  # shows the learnt parameters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First we grab the learnt parameters from the above run\n",
      "k = m.kern\n",
      "noisevar = m['noise_variance']\n",
      "# Now we do the full fit, iterating in patches over the whole area.\n",
      "# NOTE: running this code block will still take a while, but nowhere near as long as trying to push it all through one grand inference.\n",
      "for whichlatcen, latcen in enumerate(latcens):\n",
      "    rowcount = 0\n",
      "    rowavailcount = 0\n",
      "    for whichloncen, loncen in enumerate(loncens):\n",
      "        imgoff_lon = int((loncen - corner_lon)/resn_lon)\n",
      "        imgoff_lat = int((latcen - corner_lat)/resn_lat)\n",
      "        lonmin = loncen - (patchsize * 0.5)\n",
      "        lonmax = loncen + (patchsize * 0.5)\n",
      "        latmin = latcen - (patchsize * 0.5)\n",
      "        latmax = latcen + (patchsize * 0.5)\n",
      "        curpoints = filter(lambda p: p[0]>lonmin and p[0]<lonmax and p[1]>latmin and p[1]<latmax, allpoints)\n",
      "        if len(curpoints)==0:\n",
      "            continue\n",
      "        np.random.shuffle(curpoints)\n",
      "        rowavailcount += len(curpoints)\n",
      "        curpoints = curpoints[:maxpointsperpatch]\n",
      "        rowcount += len(curpoints)\n",
      "        # fit a gp to this area\n",
      "        Xin = np.array([[item[0], item[1]] for item in curpoints])\n",
      "        Yin = np.array([[item[2]]          for item in curpoints])\n",
      "        onem = GPy.models.GPRegression(Xin, Yin, k)\n",
      "        onem['noise_variance'] = noisevar\n",
      "        # get a grid\n",
      "        alongrid = np.arange(lonmin, lonmax, resn_lon)\n",
      "        alatgrid = np.arange(latmin, latmax, resn_lat)\n",
      "        agrid = []\n",
      "        for lat in alatgrid:\n",
      "            agrid.append(onem.predict(np.array([[lon, lat] for lon in alongrid]))[0].flatten())\n",
      "        # add our grid onto the grand one\n",
      "        agrid = np.array(agrid).T\n",
      "        #print agrid.shape\n",
      "        agrid *= window2d\n",
      "        patchedimage[imgoff_lon:imgoff_lon+np.shape(agrid)[0],  imgoff_lat:imgoff_lat+np.shape(agrid)[1]] += agrid\n",
      "    print \"Done row %i/%i. Used %i points out of a potential %i.\" % (whichlatcen, len(latcens), rowcount, rowavailcount)\n",
      "print \"Done.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = pb.figure()\n",
      "fig.set_size_inches(20, 30)\n",
      "pb.imshow(np.maximum(0, patchedimage.T * ele_scaler), origin='lower', interpolation='nearest') #, vmin=-10)\n",
      "pb.xticks([0, patchedimage.shape[0]-1], [corner_lon - patchsize * 0.5, corner_lon + totsize_lon + patchsize * 0.5])\n",
      "pb.yticks([0, patchedimage.shape[1]-1], [corner_lat - patchsize * 0.5, corner_lat + totsize_lat + patchsize * 0.5])\n",
      "pb.set_cmap('gist_earth')\n",
      "pb.colorbar()\n",
      "pb.title(\"Elevation map of Britain, estimated from OpenStreetMap point data, using Gaussian Processes (GPy). (This image (c) Dan Stowell, CC-BY. The underlying data is (c) OpenStreetMap contributors, ODbL.)\", fontsize=\"small\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}